import glob
import json
import os
import pickle
import random
import re
from collections import Counter
from datetime import datetime

import matplotlib.pyplot as plt
import numpy
import numpy as np
from flask import (Flask, Response, jsonify, make_response, render_template,
                   request, send_file, url_for)
from keras.callbacks import ModelCheckpoint
from keras.layers import LSTM, Activation
from keras.layers import BatchNormalization as BatchNorm
from keras.layers import Dense, Dropout
from keras.models import Sequential, load_model
from keras.utils import np_utils
from music21 import *
from music21 import chord, converter, instrument, note, stream
from numpy import load, save

app = Flask(__name__)
app.config.from_object(__name__)


@app.route("/")
def init():
    return render_template("index.html")


counter = 0


@app.route('/generate/', methods=['POST'])
def generate():
    global counter
    counter = counter + 1
    jsonl = json.loads(request.json)
    duration = int(int(jsonl['duration']) * 50 / 12)
    generate(duration, counter)
    # new_file = open('newMusic.mid', 'rb')
    # print(new_file)
    # return send_file(new_file, mimetype='audio/mid', as_attachment=True, attachment_filename='newMusic.mid')
    return "newMusic_" + str(counter) + ".mid"


def create_midi(prediction_output, counter):
    # Convert the predicted notes into midi file

    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 0.5
    # print(output_notes)
    midi_stream = stream.Stream(output_notes)

    midi_stream.write('midi', fp='static/music/newMusic_' +
                      str(counter) + '.mid')


def generate_notes(model, network_input, pitchnames, n_vocab, notes_number):
    # Pick a random sequence as an input for the prediction
    start = numpy.random.randint(0, len(network_input)-1)

    int_to_note = dict((number, note)
                       for number, note in enumerate(pitchnames))

    pattern = network_input[start]
    prediction_output = []

    # generate a number of notes
    for note_index in range(notes_number):
        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))
        prediction_input = prediction_input / float(n_vocab)

        prediction = model.predict(prediction_input, verbose=0)

        index = numpy.argmax(prediction)
        result = int_to_note[index]
        prediction_output.append(result)

        pattern.append(index)
        pattern = pattern[1:len(pattern)]

    return prediction_output


def create_network(network_input, n_vocab):
    # Create the model and load the weights into it
    model = Sequential()
    model.add(LSTM(
        512,
        input_shape=(network_input.shape[1], network_input.shape[2]),
        recurrent_dropout=0.3,
        return_sequences=True
    ))
    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))
    model.add(LSTM(512))
    model.add(BatchNorm())
    model.add(Dropout(0.3))
    model.add(Dense(256))
    model.add(Activation('relu'))
    model.add(BatchNorm())
    model.add(Dropout(0.3))
    model.add(Dense(n_vocab))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

    # Load the weights (model) since we trained it in the training phase
    model.load_weights('newModel.hdf5')

    return model


def prepare_sequences(notes, pitchnames, n_vocab):
    # Mapping between notes and integers and back
    note_to_int = dict((note, number)
                       for number, note in enumerate(pitchnames))

    sequence_length = 100
    network_input = []
    output = []
    for i in range(0, len(notes) - sequence_length, 1):
        sequence_in = notes[i:i + sequence_length]
        sequence_out = notes[i + sequence_length]
        network_input.append([note_to_int[char] for char in sequence_in])
        output.append(note_to_int[sequence_out])

    n_patterns = len(network_input)

    normalized_input = numpy.reshape(
        network_input, (n_patterns, sequence_length, 1))

    normalized_input = normalized_input / float(n_vocab)

    return (network_input, normalized_input)


def generate(notes_number, counter):

    # Load the notes used to train the model, to use them in generating new music
    with open('notes', 'rb') as filepath:
        notes = pickle.load(filepath)

    # Get all pitch names
    pitchnames = sorted(set(item for item in notes))
    # Get all pitch names
    n_vocab = len(set(notes))

    network_input, normalized_input = prepare_sequences(
        notes, pitchnames, n_vocab)

    # Get the model to use it in generating new music
    model = create_network(normalized_input, n_vocab)

    # Generate new music
    prediction_output = generate_notes(
        model, network_input, pitchnames, n_vocab, notes_number)

    # Convert the generated music into midi file
    create_midi(prediction_output, counter)


if __name__ == "__main__":
    app.run()
